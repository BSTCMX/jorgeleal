{
  "order": 7,
  "title": "BSTCGenerator",
  "descriptionEs": "Plataforma inteligente de generación de audio con capacidades de aprendizaje automático, diseñada para productores musicales. El sistema analiza patrones de preferencias del usuario mediante técnicas de ML para optimizar futuras generaciones, operando sobre una arquitectura híbrida de costo-cero.\n\nSistema de Aprendizaje Automático (Machine Learning): Pattern Recognition Engine que extrae automáticamente características de prompts exitosos (BPM, instrumentación, keywords de género/mood) usando procesamiento de lenguaje natural y análisis estadístico. Audio Embeddings con vectorización de audio usando librosa para extraer características tímbricas, armónicas y espectrales (MFCC, chroma, spectral features), implementando búsqueda por similitud coseno en espacio latente de 30 dimensiones. Adaptive User Profiling con algoritmo de construcción de perfil en tiempo real que actualiza preferencias automáticamente tras cada feedback, calculando métricas como tasa de éxito, rangos preferidos y palabras clave correlacionadas. Recommendation System basado en análisis de embeddings históricos para predecir probabilidad de éxito de nuevas generaciones.\n\nFrontend: Vanilla JS, HTML5, CSS3 implementando SPA (Single Page Application) sin frameworks. Sistema de paginación client-side para manejo eficiente de datasets grandes. Dashboard de insights en tiempo real con visualización de patrones aprendidos. UI responsive y minimalista optimizada para flujo de trabajo de producción.\n\nBackend & Arquitectura: FastAPI como orquestador central RESTful, gestionando toda la lógica de negocio. Arquitectura Híbrida Desacoplada con separación de componentes: Centro de Comando (GitHub Codespaces) para backend, base de datos y sistema de aprendizaje; Worker de Generación (Google Colab) con GPU dedicada para inferencia de MusicGen; comunicación vía API REST con tunnel ngrok para conexión segura.\n\nSistema de Persistencia: SQLite con esquema optimizado para almacenar metadatos de generaciones, embeddings de audio (vectores serializados como BLOB), feedback binario para construcción de dataset de entrenamiento, y estadísticas agregadas. Sistema de versionado Git para persistencia del conocimiento entre sesiones. Arquitectura eficiente: embeddings permanentes en BD (~2KB), archivos MP3 como artefactos transitorios (~1.2MB).\n\nIA & Modelos: MusicGen (Meta Audiocraft) para generación text-to-music en GPU con capacidades melody-guided para re-interpretación de audio existente manteniendo estructura melódica. Gemini 2.0 Flash (LLM) para asistencia opcional en refinamiento técnico de prompts. Sistema dual de generación: modo directo (prompts sin modificación para máximo control) y modo asistido (optimización automática vía Gemini).\n\nFeatures Clave: Generación melody-guided única (re-interpretación de audio manteniendo melodía), sistema de feedback binario para dataset curado, auto-análisis de patrones cada sesión, extracción automática de embeddings al generar, búsqueda de samples similares por características acústicas, predicción de preferencias basada en vectores históricos, paginación para historial extenso (20 items/página), workflow automatizado con scripts bash.\n\nMétricas & Análisis: Tasa de éxito en tiempo real (ratio likes/total), detección automática de BPM preferidos con cálculo de rangos y promedios, análisis de frecuencia de instrumentos mediante NLP en prompts, correlación de keywords de género/mood con feedback positivo, sistema de insights visuales para interpretación de patrones aprendidos.\n\nStack: Python, FastAPI, Vanilla JS (ES6+), MusicGen (Audiocraft), Gemini 2.0 Flash, SQLite, NumPy, Librosa, GitHub Codespaces, Google Colab (GPU T4), ngrok.",
  "descriptionEn": "Intelligent audio generation platform with machine learning capabilities, designed for music producers. The system analyzes user preference patterns using ML techniques to optimize future generations, operating on a zero-cost hybrid architecture.\n\nMachine Learning System: Pattern Recognition Engine that automatically extracts features from successful prompts (BPM, instrumentation, genre/mood keywords) using natural language processing and statistical analysis. Audio Embeddings with audio vectorization using librosa to extract timbral, harmonic and spectral features (MFCC, chroma, spectral features), implementing cosine similarity search in 30-dimensional latent space. Adaptive User Profiling with real-time profile building algorithm that automatically updates preferences after each feedback, calculating metrics like success rate, preferred ranges and correlated keywords. Recommendation System based on historical embeddings analysis to predict success probability of new generations.\n\nFrontend: Vanilla JS, HTML5, CSS3 implementing SPA (Single Page Application) without frameworks. Client-side pagination system for efficient handling of large datasets. Real-time insights dashboard with learned patterns visualization. Responsive and minimalist UI optimized for production workflow.\n\nBackend & Architecture: FastAPI as central RESTful orchestrator, managing all business logic. Decoupled Hybrid Architecture with component separation: Command Center (GitHub Codespaces) for backend, database and learning system; Generation Worker (Google Colab) with dedicated GPU for MusicGen inference; communication via REST API with ngrok tunnel for secure connection.\n\nPersistence System: SQLite with optimized schema to store generation metadata, audio embeddings (vectors serialized as BLOB), binary feedback for training dataset construction, and aggregated statistics. Git versioning system for knowledge persistence between sessions. Efficient architecture: permanent embeddings in DB (~2KB), MP3 files as transient artifacts (~1.2MB).\n\nAI & Models: MusicGen (Meta Audiocraft) for text-to-music generation on GPU with melody-guided capabilities for re-interpretation of existing audio while maintaining melodic structure. Gemini 2.0 Flash (LLM) for optional assistance in technical prompt refinement. Dual generation system: direct mode (unmodified prompts for maximum control) and assisted mode (automatic optimization via Gemini).\n\nKey Features: Unique melody-guided generation (audio re-interpretation maintaining melody), binary feedback system for curated dataset, auto-analysis of patterns each session, automatic embeddings extraction when generating, similar samples search by acoustic features, preference prediction based on historical vectors, pagination for extensive history (20 items/page), automated workflow with bash scripts.\n\nMetrics & Analysis: Real-time success rate (likes/total ratio), automatic detection of preferred BPMs with range and average calculation, instrument frequency analysis via NLP in prompts, genre/mood keywords correlation with positive feedback, visual insights system for learned patterns interpretation.\n\nStack: Python, FastAPI, Vanilla JS (ES6+), MusicGen (Audiocraft), Gemini 2.0 Flash, SQLite, NumPy, Librosa, GitHub Codespaces, Google Colab (GPU T4), ngrok.",
  "video": "/videos/music1.webm",
  "poster": "/images/posters/musicposter.webp",
  "tags": ["FastAPI", "MusicGen", "ML", "Gemini AI", "Librosa", "NumPy"]
}
